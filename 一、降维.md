# 降维

[toc]

## 奇异值分解

### 用向量表示数据

数据集通常被表示成矩阵 $A\in\R^{n\times d}$，不妨记 $A = \begin{bmatrix}\vec a_1^\intercal \\\vec a_2^\intercal\\\vdots\\\vec a_n^\intercal\end{bmatrix}$，每个列向量 $\vec a_i\in\R^d$ 都是一个数据

> 虽然老师上课原话 $\vec a_i$ 是行向量，但是作者认为行向量是一个非常不自然的东西，就擅自将它们都专职为列向量

**奇异值分解**会找到一个 *最适合*（"best-fitting"）的 $k$ 维 **线性子空间**

*最适合* 这里指这些点到这个子空间距离的平方和最小，就跟最小二乘法一样。也就是说找到一个线性子空间 $S$，使得：
$$
S = \arg\min_{S \text{ is a } k-\text{dim subspace }}\sum_{i = 1}^n{\rm dist}^2(\vec a_i, S)
$$
> $\arg$ 表示主值，这里 $\arg\min_S f(S)$ 表示当 $f(S)$ 取到最小值时 $S$ 的值，有多个最小值时任取一个

考虑勾股定理，这里有 ${\rm dist}^2(\vec a_i, S) + {\rm length}^2({\rm proj}(\vec a_i, S)) = {\rm length}^2(\vec a_i)$，而 ${\rm length}(\vec a_i)$ 是固定值，所以 $S$ 的一个等价表述为：
$$
\begin{aligned}
S &= \arg\min_{S \text{ is a } k-\text{dim subspace }}\sum_{i = 1}^n{\rm dist}^2(\vec a_i, S) \\
&= \arg\min_{S \text{ is a } k-\text{dim subspace }}\left(\sum_{i = 1}^n{\rm length}^2(a_i) - \sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S))\right)\\
&= \arg\max_{S \text{ is a }k-\text{dim subspace}}\sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S))
\end{aligned}
$$

### 奇异向量与奇异值

回忆线性代数中的 **范数**。若 $p\geq 1$，则记 $p$ 范数为 $\norm{\vec a}_p = \left(\sum_{i = 1}^n\abs{\vec a_i}^p\right)^{\frac 1p}$。那么有：
$$
\norm{\vec a}_2 = \sqrt{\sum_{i = 1}^na_i^2}\\
\norm{\vec a}_1 = \sum_{i = 1}^n\abs{a_i}\\
\norm{\vec a}_\infty = \max_{i = 1}^n\abs{a_i}\\
$$
在本章节，范数默认为 $2$ 范数，即默认 $\norm{\vec a} = \norm{\vec a}_2$。

（额外地，定义 $\norm{\vec a}_0$ 是 $\vec a$ 中非零元的数目）

注意到前文奇异值分解要找到的是线性子空间，也就是说这个子空间一定是过原点的

当 $k = 1$ 时，任意的一维线性子空间都可以被表述成 $1$ 个非零向量 $\vec v\in\R^d$ 张成的空间，不妨假设 $\vec v$ 的长度 $\norm{\vec v}_2$ 为 $1$。要找到这个子空间，就等价于找到这个单位向量 $\vec v$

记两个列向量 $\vec u, \vec v\in\R^d$ 的内积为 $\vec u^\intercal \vec v$ 或 $\left\langle\vec u, \vec v\right\rangle$

在这里记 ${\rm proj}(\vec a_i, \vec v) = {\rm proj}(\vec a_i, S)$，那么有 ${\rm length}({\rm proj}(\vec a_i, \vec v)) = \left\langle \vec a_i^\intercal, \vec v\right\rangle$（想想内积的几何性质），所以：
$$
\sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S)) = \sum_{i = 1}^n\left\langle \vec a_i^\intercal, \vec v\right\rangle^2 = \sum_{i = 1}^n\left(\sum_{j = 1}^na_{i, j}v_j\right)^2 = \norm{A\vec v}_2^2
$$
记 $A$ 的第一个右奇异向量 $\vec v_1 = \arg\max_{\norm{\vec v}_2 = 1}\norm{A\vec v}_2$，同时记 $A$ 的第一个奇异值 $\sigma_1(A) = \norm{A\vec v_1}_2$，那么 $\vec v_1$ 张成的线性子空间即为所求，而且所有数据向量在 $\vec v_1$ 张成的线性子空间上投影平方和为 $\sigma_1^2(A)$

当 $k = 2$ 时，一个 *贪心* 的做法是在 $\vec v_1$ 张成的子空间基础上再新增一个维度，也就是再找一个向量 $\vec v_2\perp \vec v_1$，然后取 $S = {\rm span}(\vec v_1, \vec v_2)$。所以我们先定义 $A$ 的第二个右奇异向量与第二个奇异值：
$$
\vec v_2 = \arg\max_{\norm{\vec v}_2 = 1, \vec v\perp \vec v_1}\norm{A\vec v}_2\\
\sigma_2(A) = \norm{A\vec v_2}_2
$$
后续再证明这样贪心定义的右奇异向量确实能张成前文中的最优子空间

类似地，可以定义第 $i$ 个右奇异向量与第 $i$ 个奇异值：
$$
\vec v_i = \arg\max_{\norm{\vec v}_2 = 1, \vec v_i \perp \vec v_1, \cdots, \vec v_i\perp \vec v_{i - 1}}\norm{A\vec v}_2\\
\sigma_i(A) = \norm{A\vec v_i}_2
$$
假设已有在任意给定 $\R^d$ 的子空间 $T$ 内求解 $\arg\max_{\norm{\vec v}_2 = 1, \vec v\in T}\norm{A\vec v}_2$ 的（最优化）算法，那么可以很容易地设计一个求解所有右奇异向量的算法

接下来证明以上贪心做法的最优性

**定理 1：**设 $A\in\R^{n\times d}$，贪心定义的右奇异向量为 $\vec v_1, \vec v_2, \cdots, \vec v_r$。对于任意的 $1\leq k\leq r$，取 $V_k = {\rm span}(\vec v_1, \cdots, \vec v_k)$，则 $V_k$ 确实是 *最适合* $k$ 维线性子空间，其中 $r$ 是矩阵 $A$ 的 **秩**

> 为什么这里会提到秩呢？回忆一个线性代数的性质：矩阵的零空间与行空间互为正交补。倘若定理成立，那么 $V_r$ 一定是 $A$ 的行空间（因为 $A$ 投影到 $A$ 的行空间上是没有任何长度损失的，而 $A$ 行空间的维数是 $A$ 的行秩 $r$）。这就表明所有与 $V_r$ 垂直的向量都在 $A$ 的零空间内

在证明这个定理之前，先说明一个子空间内投影的性质：

若 $\vec u_1, \vec u_2, \cdots, \vec u_k$ 是 $\R^d$ 的 $k$ 维线性子空间 $S$ 的一组单位正交基，则
$$
\sum_{i = 1}^n{\rm length}^2({\rm proj}(\vec a_i, S)) = \sum_{i = 1}^n\sum_{j = 1}^k\left\langle \vec a_i, \vec u_k\right\rangle^2 = \sum_{j =1}^k\norm{A\vec u_j}_2^2
$$
这两个等号成立的理由与计算 $\vec v_1$ 时的推导类似。

**证明：**

采用数学归纳法

$k = 1$ 的时候，联想 $\vec v_1$ 的定义我们没有贪心，自然 $V_1$ 是线性子空间

下尝试论证 $V_{k - 1}$ 是 *最适合* 线性子空间可以推得 $V_k$ 是 *最适合* 线性子空间。设某个 *最适合* $k$ 维线性子空间是 $W$，接下来尝试证 $V_k = W$

因为 $W$ 是 $k$ 维线性空间，所以 $W$ 存在一组大小为 $k$ 的单位正交基 $\vec w_1, \vec w_2, \cdots, \vec w_k$ 且满足 $\vec w_k\perp \vec v_1, \vec v_2, \cdots, \vec v_{k - 1}$，其中 $\vec v_1, \vec v_2, \cdots, \vec v_{k - 1}$ 是贪心算法算出的前 $k - 1$ 个右奇异向量

由归纳假设，因为 $V_{k - 1}$ 是 *最适合* 线性子空间，也就是说 $A$ 在 $V_{k - 1}$ 上的投影不会比 $A$ 在 $\qty{\vec w_1, \vec w_2, \cdots, \vec w_{k - 1}}$ 张成的子空间上的投影更短，联想前文所述投影的性质，有：
$$
\begin{aligned}
&\norm{A\vec w_1}^2 + \norm{A\vec w_2}^2 + \cdots + \norm{A\vec w_{k - 1}}^2\\
\leq &\norm{A\vec v_1}^2 + \norm{A\vec v_2}^2 + \cdots + \norm{A\vec v_{k - 1}}^2
\end{aligned}
$$
联想为贪心算法构造的第 $k$ 个右奇异向量 $\vec v_k$ 的定义：
$$
\vec v_k = \arg\max_{\norm{\vec v}_2 = 1, \vec v_i \perp \vec v_1, \cdots, \vec v_i\perp \vec v_{i - 1}}\norm{A\vec v}_2
$$
$\vec v_k$ 满足 $\norm{A\vec v_k}$ 是所有垂直于 $\vec v_1, \vec v_2, \cdots, \vec v_{k - 1}$ 的单位向量 $\vec v$ 中 $\norm{A\vec v}$ 最大的一个，而且 $\vec w_k$ 也是一个垂直于 $\vec v_1, \vec v_2, \cdots, \vec v_{k - 1}$ 的向量，所以
$$
\norm{A\vec w_k}_2^2\leq \norm{A\vec v_k}_2^2
$$
两个不等式相加知：
$$
\begin{aligned}
&\norm{A\vec w_1}^2 + \norm{A\vec w_2}^2 + \cdots + \norm{A\vec w_k}^2\\
\leq &\norm{A\vec v_1}^2 + \norm{A\vec v_2}^2 + \cdots + \norm{A\vec v_k}^2
\end{aligned}
$$
联想投影的性质，上式说明 $k$ 维子空间 $V_k$ 不会比 $W$ 更 *不适合*，而 $W$ 已经是 *最适合* 的 $k$ 维子空间了，所以 $V_k$ 也是一个 *最适合* 的 $k$ 维子空间

### 奇异值与 Frobenious 范数

首先定义矩阵 $A\in\R^{n\times m}$ 的 **Frobenious范数** 为矩阵中所有元素的平方和：
$$
\norm{A}_F = \sqrt{\sum_{i = 1}^n\sum_{j = 1}^ma_{i, j}^2}
$$
那么我们不难证明

**定理：**矩阵 Frobenious 范数的平方等于其全体奇异值的平方和
$$
\norm{A}_F^2 = \sum_{i = 1}^r\sigma_i^2(A)
$$

**证明：**考虑 $A$ 的 $n$ 个行向量

一方面，在自然基下使用勾股定理：
$$
\sum_{j = 1}^n\norm{\vec a_j}^2 = \sum_{j = 1}^n\sum_{k = 1}^da_{jk}^2 = ||A||_F^2
$$
另一方面，考虑到 $\vec v_1, \vec v_2, \cdots, \vec v_r$ 是 $A$ 行空间的一组单位正交基，对这组基使用勾股定理：
$$
\sum_{j = 1}^n\norm{\vec a_j}^2 = \sum_{j = 1}^n\sum_{i = 1}^r\left\langle\vec a_j,\vec v_i\right\rangle^2 = \sum_{i = 1}^r\norm{A\vec v_i}^2
$$

联立以上两式便得到原定理

### 奇异值分解

设 $A\in\R^{n\times d}$ 有右奇异向量 $\vec v_1, \vec v_2, \cdots, \vec v_r$ 与对应的奇异值 $\sigma_1(A), \sigma_2(A), \cdots, \sigma_r(A)$。定义 $A$ 的 $r$ 个左奇异向量：
$$
\vec u_i = \frac{A\vec v_i}{\sigma_i(A)}
$$
因为奇异值的定义是 $\sigma_i(A) = \norm{A\vec v_i}_2$，所以不难看出左奇异向量也是单位向量

**引理：**左奇异向量两两正交

**证明：**反证

> 我们该从什么角度引出矛盾呢？$\vec u_i$ 是被 $\vec v_i$ 定义的，而 $\vec v_i$ 是被一个函数的最大值点 $\arg\max_{\norm{\vec v_i} = 1, \vec v_i\perp V_{i - 1}}\norm{A\vec v_i}_2$ 定义的，以下便试图从引理不成立推到某 $\vec v_i$ 没有取到最大值点。假如引理不成立可以推得 $\vec v_i$ 确实没有取到函数的最大值点而 $\vec v'$ 才是，考虑到以上最优化问题有约束（$\vec v_i$ 被限定在垂直于 $V_{i - 1}$ 的方向上），说明 $\vec v'$ 理应是 $\vec v_{i}, \vec v_{i + 1}, \cdots, \vec v_r$ 的线性组合。而引理不成立能等价于描述一对向量 $\vec u_i, \vec u_j$ 不垂直，并没有提供其他向量的有用信息，所以下文就试图将 $\vec v_i$ 与 $\vec v_j$ 来线性组合出一个 $\vec v'$ 满足 $\norm{A\vec v'}_2 > \norm{A\vec v_i}_2$。

假设存在某些左奇异向量两两不正交，那么存在 $1\leq i < j\leq r$ 满足 $\vec u_i\not\perp \vec u_j$

**不失一般性**，假设 $\left\langle \vec u_i, \vec u_j\right\rangle > 0$，记 $\delta\triangleq \left\langle\vec u_i, \vec u_j\right\rangle$

> 为什么这个假设“不失一般性”呢？读者若不理解，可以将这视为分类讨论，然后尝试用几乎完全相同的方法证明 $\left\langle \vec u_i, \vec u_j\right\rangle < 0$ 的场景，注意后文哪些地方正确性依赖于是否有 $\delta > 0$

对于 $\epsilon\in\R$，设
$$
\vec v'(\epsilon) = \frac{\vec v_i + \epsilon \vec v_j}{\norm{\vec v_i + \epsilon \vec v_j}_2}
$$
>  简记 $\vec v'(\epsilon)$ 为 $\vec v'$，但读者请记住 $\vec v'$ 随 $\epsilon$ 变化而变化

则有：

$$
\norm{\vec v_i + \epsilon\vec v_j} = \sqrt{\left\langle\vec v_i + \epsilon \vec v_j, \vec v_i + \epsilon\vec v_j\right\rangle} = \sqrt{1 + \epsilon^2}\\
A\vec v' = \frac{A\vec v_i + \epsilon A\vec v_j}{\norm{\vec v_i + \epsilon \vec v_j}_2} = \frac{\sigma_i\vec u_i + \epsilon\sigma_j \vec u_j}{\sqrt{1 + \epsilon^2}}
$$
注意到 $\vec u_i$ 是单位向量，而且由柯西不等式有：
$$
\norm{A\vec v'}_2 = \norm{\vec u_i}_2\norm{A\vec v'}_2 \geq \vec u_i^\intercal A\vec v'
$$
再代入上面对 $A\vec v'$ 的分析与反证假设 $\vec u_i^\intercal\vec u_j = \delta > 0$ 有：
$$
\norm{A\vec v'}_2\geq \vec u_i^\intercal \frac{\sigma_iu_i + \epsilon\sigma_j\vec u_j}{\sqrt{1 + \epsilon^2}} = \frac{\sigma_i + \epsilon\delta\sigma_j}{\sqrt{1 + \epsilon^2}}
$$
> $\epsilon$ 是我们可以任取的，我们希望取定某个满足 $\norm{A\vec v'} > \norm{A\vec v}$ 的 $\epsilon$。当 $\epsilon\leq 0$ 的时候，聪明的你应该可以看出 $\norm{A\vec v'}_2 \leq \sigma_i$，所以我们限定 $\epsilon > 0$ 再进行进一步分析

考虑到 $\sigma_i, \delta, \sigma_j > 0$，当 $\epsilon > 0$ 时，代入不等式 $(1 - \epsilon^2 / 2)^2(1 + \epsilon^2)< 1$

> 这个不等式可以由 $(1 + \epsilon^2)^{-\frac 12}$ 在 $\epsilon = 0$ 处泰勒展开得到

$$
\norm{A\vec v'}_2\geq \frac{\sigma_i + \epsilon\delta\sigma_j}{\sqrt{1 + \epsilon^2}} > (\sigma_i + \epsilon\delta\sigma_j)\qty(1 - \frac{\epsilon^2}2) = \sigma_i +  \epsilon\qty(\sigma_j\delta - \frac{\sigma_i}2\epsilon - \frac{\sigma_j\delta}2\epsilon^2)
$$
只要 $\epsilon$ 充分小，$\sigma_j\delta - \frac{\sigma_i}2\epsilon - \frac{\sigma_j\delta}2\epsilon^2$ 总能变成一个正数，这表明 $\epsilon$ 充分小时 $\norm{A\vec v'}_2 > \sigma_i$ 成立，取定一个这样充分小的 $\epsilon$ 与其对应的 $\vec v'$

> 当然也可以简单地写：求导得
>$$
> \eval{\dv{\norm{A\vec v'}_2}{\epsilon}}_{\epsilon = 0} > 0
> $$
> 只要 $\epsilon$ 足够小就有 $\norm{A\vec v'(\epsilon)}_2 > \norm{A\vec v'(0)}_2 = \norm{A\vec v_i}$，取定一个这样的 $\epsilon$ 与 $\vec v'$

另一方面，$\vec v'$ 是 $\vec v_i$ 与 $\vec v_j$ 的线性组合，而 $\vec v_i, \vec v_j$ 均与 $\vec v_1, \vec v_2, \cdots, \vec v_{i - 1}$ 垂直，这就表明 $\vec v'$ 也与 $\vec v_1, \vec v_2, \cdots, \vec v_{i - 1}$ 垂直。而 $\norm{A\vec v'}_2 > \norm{A\vec v_i}_2$，这与 $\vec v_i$ 的定义矛盾

>  $\vec v_i$ 是所有与 $\vec v_1, \vec v_2, \cdots, \vec v_{i - 1}$ 垂直的单位向量 $v$ 中 $\norm{A\vec v}_2$ 最大的一个，然而 $\vec v'$ 不但满足单位向量条件与垂直条件，$\norm{A\vec v'}_2$ 还比 $\norm{A\vec v_i}_2$ 更大

故假设不成立

<hr>

有了以上的准备工作，接下来我们便可以证明

**定理：**
$$
A = \sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal
$$

**证明：**

> 老师认为这个技巧大家需要掌握

首先注意到一个事实：若 $B, C\in\R^{n\times d}$，则：
$$
B = C\iff B\vec v = C\vec v,\forall v\in\R^d
$$
为了证明定理成立，只要证明：对于任意的 $\vec v\in\R^d$，都有
$$
A\vec v = \qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\vec v
$$
下证明上式。首先注意到对于一类特殊的向量——$A$ 的 $r$ 个右奇异向量均有：
$$
A\vec v_i = \sigma_i\vec u_i = \qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\vec v_i
$$
>  第二个等号成立是因为 $A$ 的右奇异值两两正交 $j\neq i\implies \vec v_j^\intercal \vec v_i = 0$

但是考虑到 $r\leq d$，也就是不一定有 $r = d$。所以我们先补齐右奇异向量，任意地将 $\vec v_1, \vec v_2, \cdots, \vec v_r$ 补齐成 $\R^d$ 的一组单位正交基 $\vec v_1, \vec v_2, \cdots, \vec v_d$，也就是选择单位向量 $\vec v_{r + 1}, \cdots, \vec v_d$ 满足 $\vec v_1, \vec v_2, \cdots, \vec v_d$ 两两正交

补齐了之后就可以在这组基上展开 $\vec v$：设 $\left\langle\vec v, \vec v_i\right\rangle = \alpha_i$，则
$$
\vec v = \sum_{i = 1}^d\alpha_i\vec v_i
$$
因为 $A$ 的秩为 $r$，而 $\vec v_1, \vec v_2, \cdots, \vec v_r$ 恰构成 $A$ 的行空间，所以 $A\vec v_{r + 1} = A\vec v_{r + 2} = \cdots = A\vec v_d = \vec 0$。所以
$$
A\vec v = A\sum_{i = 1}^d\alpha_i\vec v_i = \sum_{i = 1}^d\alpha_iA\vec v_i = \sum_{i = 1}^r\alpha_i\sigma_i\vec u_i\\
\qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\vec v = \qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\qty(\sum_{i = 1}^d\alpha_i\vec v_i) = \sum_{i = 1}^r\sum_{j = 1}^d\sigma_i\alpha_j\vec u_i\vec v_i^\intercal\vec v_j = \sum_{i = 1}^r\sigma_i\alpha_i\vec u_i = A\vec v
$$
对于任意的 $\vec v\in\R^d$ 上式均成立，由前文提及的事实，定理证毕

记
$$
U\triangleq \begin{bmatrix}\vec u_1 & \vec u_2 & \cdots & \vec u_r\end{bmatrix}\in\R^{n\times r}, \Sigma \triangleq{\rm diag}\qty(\sigma_1, \sigma_2, \cdots, \sigma_r)\in\R^{r\times r}, V\triangleq\begin{bmatrix}\vec v_1 & \vec v_2 & \cdots & \vec v_r\end{bmatrix}\in \R^{d\times r}
$$
由上定理知
$$
A = U\Sigma V^\intercal
$$
这称为矩阵 $A$ 的奇异值分解

### 奇异向量与特征向量

如果 $B$ 是 $n$ 阶对称方阵，即 $B = B^\intercal, B\in\R^{n\times n}$，则 $B$ 有 $n$ 个单位正交特征向量 $\set{\vec v_i}$ 和它们对应的特征值 $\set{\lambda_i}$

对于一个一般的矩阵 $A\in\R^{n\times d}$，如果 $A$ 的奇异值分解可以写成 $\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal$，则：

* $A\vec v_i = \sigma_i\vec u_i$，因为这是 $\vec u_i$ 的定义
* $A^\intercal \vec u_i = \sigma_i\vec v_i$，因为前文引理得到 $\set{\vec u_i}$ 两两正交，代入奇异值分解表达式即可

对于任意矩阵 $A\in\R^{n\times d}$，定义其协方差矩阵 $B = A^\intercal A\in\R^{d\times d}$，则

$$
B\vec v_i = A^\intercal A\vec v_i = A^\intercal (A\vec v_i) = A^\intercal(\sigma_i\vec u_i) = \sigma_i^2\vec v_i
$$

这表明 $A$ 的任意右奇异向量 $\vec v_i$ 都是对称方阵 $B = A^\intercal A$ 的特征向量，该特征向量对应的特征值为 $\sigma_i^2$

同样地，$A$ 的任意左奇异向量 $\vec u_i$ 都是对称方阵 $AA^\intercal$ 的特征向量，对应的特征值也是 $\sigma_i^2$

### 低秩近似

奇异值分解可以找到一个与原矩阵相近，但秩较低的近似矩阵。

为什么要进行低秩近似呢？假设有 $n$ 个客户看过 $d$ 部电影，数据集 $A\in\R^{n\times d}$ 第 $i$ 行第 $j$ 列的元素 $a_{ij}$ 表示第 $i$ 个人给第 $j$ 个电影打多少分。这可能是一个很大的矩阵，但是如果客户给电影的评分只有 $k$ 个因素（例如 HE/BE，电影题材，特效 etc），而每个客户的评分是对这 $k$ 个因素的线性叠加，那么可以认为 $A = XY, X\in\R^{n\times k}, Y\in\R^{k\times d}$，其中 $y_{kj}$ 表示第 $j$ 个电影中含有多少个第 $k$ 种因素，而 $x_{ij}$ 表示第 $i$ 个人有多喜欢第 $j$ 种因素

回忆起 $A = \sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal$，那么我们可以考虑选择 $U_k = \begin{bmatrix}\vec u_1 &\vec u_2 & \cdots &\vec u_k\end{bmatrix},$ $D_k = {\rm diag}(\sigma_1, \sigma_2, \cdots, \sigma_k),$ $ V_k = \begin{bmatrix}\vec v_1 & \vec v_2 & \cdots & \vec v_k\end{bmatrix}$，那么我们取 $X = U_kD_k,$, $Y = V_k^\intercal$，那么 $A_k \triangleq XY = \sum_{i = 1}^k\sigma_iu_iv_i^\intercal$ 可以合理地作为 $A$ 的一个秩为 $k$ 的近似

使用奇异值分解进行低秩近似有一些理论意义：设 $B\in\R^{n\times d}$ 是任意秩为 $k$ 的矩阵，如果以 Frobenious 范数 $\norm{A - B}_F$ 来衡量 $B$ 是否是 $A$ 的一个好近似，越低越好，则上述 $A_k$ 就是 $A$ 一个最好的近似。当以 $2$ 范数 $\norm{A - B}_2$ 来衡量时也是 $A_k$ 也时最好的

首先我们证明 Frobenious 范数意义下 $A_k$ 是最佳近似

**引理：**$A_k$ 的第 $i$ 行是 $A$ 第 $i$ 行到 $\text{span}\set{\vec v_1, \vec v_2, \cdots, \vec v_k}$ 的投影

**证明：**设 $A = \begin{bmatrix}\vec a_1^\intercal\\\vec a_2^\intercal\\\vdots\\\vec a_n^\intercal\end{bmatrix}$，将 $A$ 的第 $i$ 行（转置成列向量） $\vec a_i$ 投影到 $\text{span}\set{\vec v_1, \vec v_2, \cdots, \vec v_k}$，设投影出向量 $\vec a_i'$，则由 $\vec v_1, \vec v_2, \cdots, \vec v_k$ 是单位正交向量知：
$$
\vec a_i' = \sum_{i = 1}^k\left\langle \vec a_i, \vec v_i\right\rangle\vec v_i = \sum_{i = 1}^k\vec v_i\vec v_i^\intercal \vec a_i = \qty(\sum_{i = 1}^k\vec v_i\vec v_i^\intercal)\vec a_i
$$
将这些投影向量转置后拼成矩阵 $A'$，则有
$$
A' = \begin{bmatrix}\vec a_1'^\intercal\\\vec a_2'^\intercal\\\vdots\\\vec a_n'^\intercal\end{bmatrix} = \begin{bmatrix}\vec a_1^\intercal\sum_{i = 1}^k\vec v_i\vec v_i^\intercal\\\vec a_2^\intercal\sum_{i = 1}^k\vec v_i\vec v_i^\intercal\\\vdots\\\vec a_n^\intercal\sum_{i = 1}^k\vec v_i\vec v_i^\intercal\end{bmatrix} = \begin{bmatrix}\vec a_1^\intercal\\\vec a_2^\intercal\\\vdots\\\vec a_n^\intercal\end{bmatrix}\sum_{i = 1}^k\vec v_i\vec v_i^\intercal = A\sum_{i = 1}^k\vec v_i\vec v_i^\intercal = \qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\qty(\sum_{i = 1}^k\vec v_i\vec v_i^\intercal) = \sum_{i = 1}^k\sigma_i\vec u_i\vec v_i^\intercal
$$
引理证毕

由以上引理，可以得到
$$
\norm{A - A_k}_F^2 = \sum_{i = 1}^n\norm{\vec a_i - \vec a_i'}_2^2 = \sum_{i = 1}^n\text{dist}^2(\vec a_i, V_k)
$$
因为奇异值分解得到的 $k$ 维子空间是最佳平方逼近的（这是由定义决定的），所以
$$
\norm{A - A_k}^2_F = \sum_{i = 1}^n\text{dist}^2(\vec a_i, V_k) = \min_{S : \dim(S) = k}\sum_{i = 1}^n\text{dist}^2(\vec a_i, S)
$$
**定理：**对于任意秩不超过 $k$ 的值 $B$，$\norm{A - A_k}_F \leq \norm{A - B}_F$

**证明：**对于任意秩不超过 $k$ 的矩阵 $B$，假设 $B$ 使 $\norm{A - B}_F^2$ 最小

设 $V$ 是 $B$ 的行空间，则 $\dim V\leq k$

$B$ 的第 $i$ 行一定是 $A$ 的第 $i$ 行到 $V$ 上的投影。若不然，将 $B$ 的第 $i$ 行改为 $A$ 的第 $i$ 行到 $V$ 的投影，则 $B$ 的行空间 $V$ 不变，但是 $\norm{A - B}_F$ 变小了

所以 $\norm{A - A_k}_F^2 = \min_{S : \dim(S) = k}\sum_{i = 1}^n\text{dist}^2(\vec a_i, S) \leq \text{dist}^2(\vec a_i, V)$

然后我们证明 $A_k$ 是 $2$ 范数意义下的最佳近似

首先我们要定义一下矩阵的 $2$ 范数：
$$
\norm{A}_2\triangleq \max_{\norm{\vec x}_2 = 1}\norm{A\vec x}_2
$$
则由奇异值的定义可知 $\norm{A}_2 = \sigma_1(A)$

**引理：**$\norm{A - A_k}_2 = \sigma_{k + 1}$

**证明：**
$$
A = \sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal\\
A_k = \sum_{i = 1}^k\sigma_i\vec u_i\vec v_i^\intercal\\
A - A_k = \sum_{i = k + 1}^r\sigma_i\vec u_i\vec v_i^\intercal
$$
跟先前一样，我们添加 $\vec v_{r + 1}, \vec v_{r + 2}, \cdots, \vec v_d$ 使得 $\set{\vec v_1, \vec v_2, \cdots, \vec v_d}$ 是 $\R^d$ 的一组单位正交基

那么对于任意的单位向量 $\vec v\in\R^d, \norm{\vec v}_2 = 1$，我们在以上的单位正交基下进行分解，设分解成 $\vec v = \sum_{i = 1}^d\alpha_i\vec v_i$，其中 $\alpha_i = \left\langle \vec v, \vec v_i\right\rangle$

则
$$
(A - A_k)\vec v = \qty(\sum_{i = k + 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\qty(\sum_{j = 1}^r\alpha_i\vec v_j) = \sum_{i = k + 1}^r\alpha_i\sigma_i\vec u_i\\
\begin{aligned}
\norm{(A - A_k)\vec v}_2 &= \norm{\sum_{i = k + 1}^r\alpha_i\sigma_i\vec u_i}_2 \\&= \sqrt{\sum_{i = k + 1}^r\alpha_i^2\sigma_i^2}\\&\leq\sqrt{\sum_{i = k + 1}^r\alpha_i^2\sigma_{k + 1}^2} \\&= \sigma_{k + 1}\sqrt{\sum_{i = k + 1}^r\alpha_i^2}\\&\leq\sigma_{k + 1}\sqrt{\sum_{i = 1}^d\alpha_i^2} \\&= \sigma_{k + 1}
\end{aligned}
$$
而且观察到 $\vec v = \vec v_k$ 时上式中的不等号全部都可以变成等于号，这就表明 $\norm{A - A_k}_2 = \max_{\norm{\vec v}_2 = 1}\norm{A\vec v}_2 = \sigma_{k + 1}$

**定理：**对于任意秩不超过 $k$ 的矩阵 $B$，$\norm{A - A_k}_2\leq \norm{A - B}_2$

**证明：**如果 $\text{rank}(A)\leq k$，则 $A = A_k$，即 $\norm{A - A_k}_2 = 0$，定理自然成立

如果 $\text{rank}(A) > k$，考虑 $B$ 的零空间 ${\rm Null}(B) = \set{\vec v | B\vec v = 0}$。由零空间与行空间互为正交补知 $\dim({\rm Null}(B))= d - {\rm rank}(B)\geq d - k$

考虑 $A$ 的前 $k + 1$ 个奇异向量张成的空间 ${\rm span}\set{\vec v_1, \vec v_2, \cdots, \vec v_{k + 1}}$，这是一个 $k + 1$ 维线性子空间，其必不与 ${\rm Null}(B)$ 正交

> 如果正交，则它们可以一起张成一个 $\dim\qty({\rm span}\set{\vec v_1, \vec v_2, \cdots, \vec v_{k + 1}}) + \dim\qty({\rm Null}(B))\geq d + 1$ 维的 $\R^d$ 子空间，这是荒谬的

所以 ${\rm Null}(B)\cap {\rm span}\set{\vec v_1, \vec v_2, \cdots, \vec v_{k + 1}}$ 是 $\R^d$ 一个维数大于等于 $1$ 的线性子空间，所以存在一个单位向量 $\vec z\in \R^d, \norm{\vec z}_2 = 1$ 满足 $\vec z\in{\rm Null}(B)$ 且 $\vec z\in{\rm span}\set{\vec v_1, \vec v_2, \cdots, \vec v_{k + 1}}$。因为 $\vec z\in {\rm Null}(B)$，所以 $B\vec z = \vec 0$。因为 $\vec z\in{\rm span}\set{\vec v_1, \vec v_2, \cdots, \vec v_{k + 1}}$，所以存在 $\alpha_1, \alpha_2, \cdots, \alpha_{k + 1}\in\R$ 满足 $\vec z = \sum_{i = 1}^{k + 1}\alpha_i\vec v_i$；因为 $\set{\vec v_1, \vec v_2, \cdots, \vec v_{k + 1}}$ 两两正交，所以由勾股定理知 $\sum_{i = 1}^{k + 1}\alpha_i^2 = \norm{\vec z}_2^2 = 1$。所以：
$$
\begin{aligned}
\norm{(A - B)\vec z} &= \norm{A\vec z - B\vec z} \\&= \norm{A\vec z} \\&= \norm{\qty(\sum_{i = 1}^r\sigma_i\vec u_i\vec v_i^\intercal)\qty(\sum_{i = 1}^{k + 1}\alpha_i\vec v_i)}\\
&= \norm{\sum_{i = 1}^{k + 1}\alpha_i\sigma_i\vec u_i}\\
&= \sqrt{\sum_{i = 1}^{k + 1}\alpha_i^2\sigma_i^2}\\
&\geq \sqrt{\sum_{i = 1}^{k + 1}\alpha_i^2\sigma_{k + 1}^2}\\
&= \sigma_{k + 1}\sqrt{\sum_{i = 1}^{k + 1}\alpha_i^2}\\
&= \sigma_{k + 1}\\
&= \norm{A - A_k}_2
\end{aligned}
$$

> 上式所使用的技巧与证明引理非常类似

所以 $\norm{A - B}_2 = \max_{\norm{\vec v} = 1}\norm{(A - B)\vec v}\geq \norm{(A - B)\vec z} = \norm{A - A_k}_2$，定理成立